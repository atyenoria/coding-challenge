{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": "# LLM Decoding from Scratch\n\nThis notebook demonstrates how to implement text generation decoding strategies from scratch using a real language model (GPT-2 small).\n\n## What We'll Build\n\n| Strategy | Description | Use Case |\n|----------|-------------|----------|\n| **Greedy** | Always pick highest probability token | Deterministic, fast |\n| **Temperature** | Scale logits before softmax | Control randomness |\n| **Top-p (Nucleus)** | Sample from smallest set with cumulative p | Dynamic vocabulary |\n| **Beam Search** | Track multiple candidates | Higher quality |\n\n---\n\n## Key Insight: LLMs Only Predict Next Token\n\n```\nInput: \"The cat sat on the\"\n                          ↓\n              [LLM computes logits]\n                          ↓\n         logits = [2.1, -0.5, 3.2, ...] (vocab_size)\n                          ↓\n              [Apply decoding strategy]\n                          ↓\n                  Next token: \"mat\"\n```\n\nThe **decoding strategy** is how we convert logits → next token."
  },
  {
   "cell_type": "code",
   "id": "8eky370tr3g",
   "source": "!pip install torch==2.1.0 transformers==4.36.0 matplotlib==3.8.2 numpy==1.26.2",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "setup-header",
   "metadata": {},
   "source": [
    "---\n",
    "## Setup: Load GPT-2 Small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load GPT-2 small (124M parameters - smallest real LLM)\n",
    "print(\"Loading GPT-2 small...\")\n",
    "model_name = \"gpt2\"  # gpt2 = gpt2-small (124M params)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "model.eval()  # Set to evaluation mode\n",
    "\n",
    "# Move to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "print(f\"Model loaded on: {device}\")\n",
    "print(f\"Vocabulary size: {tokenizer.vocab_size}\")\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "logits-header",
   "metadata": {},
   "source": [
    "---\n",
    "## Understanding Logits\n",
    "\n",
    "The model outputs **logits** - raw scores for each token in the vocabulary.\n",
    "\n",
    "```\n",
    "logits[i] = how much the model \"likes\" token i as the next token\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "logits-demo",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next_token_logits(prompt: str) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Get logits for the next token given a prompt.\n",
    "    \n",
    "    Returns: tensor of shape (vocab_size,)\n",
    "    \"\"\"\n",
    "    # Tokenize input\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    # Forward pass (no gradient needed for inference)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids)\n",
    "        # outputs.logits shape: (batch=1, seq_len, vocab_size)\n",
    "        # We want the logits for the LAST position (next token prediction)\n",
    "        logits = outputs.logits[0, -1, :]  # shape: (vocab_size,)\n",
    "    \n",
    "    return logits\n",
    "\n",
    "# Demo: Get logits for a prompt\n",
    "prompt = \"The capital of France is\"\n",
    "logits = get_next_token_logits(prompt)\n",
    "\n",
    "print(f\"Prompt: '{prompt}'\")\n",
    "print(f\"Logits shape: {logits.shape}\")\n",
    "print(f\"Logits range: [{logits.min():.2f}, {logits.max():.2f}]\")\n",
    "\n",
    "# Show top 10 tokens by logit value\n",
    "top_k_logits, top_k_indices = torch.topk(logits, 10)\n",
    "print(\"\\nTop 10 tokens by logit:\")\n",
    "for i, (logit, idx) in enumerate(zip(top_k_logits, top_k_indices)):\n",
    "    token = tokenizer.decode([idx])\n",
    "    print(f\"  {i+1}. '{token}' (logit: {logit:.2f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "greedy-header",
   "metadata": {},
   "source": [
    "---\n",
    "## Strategy 1: Greedy Decoding\n",
    "\n",
    "**Simplest strategy**: Always pick the token with the highest logit.\n",
    "\n",
    "```python\n",
    "next_token = argmax(logits)\n",
    "```\n",
    "\n",
    "**Pros**: Deterministic, fast  \n",
    "**Cons**: Repetitive, boring text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "greedy",
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_decode(prompt: str, max_new_tokens: int = 50) -> str:\n",
    "    \"\"\"\n",
    "    Greedy decoding: always pick the highest probability token.\n",
    "    \n",
    "    Algorithm:\n",
    "    1. Get logits for next token\n",
    "    2. Pick token with highest logit (argmax)\n",
    "    3. Append to sequence\n",
    "    4. Repeat\n",
    "    \"\"\"\n",
    "    # Start with the prompt tokens\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
    "    generated_ids = input_ids[0].tolist()  # Convert to list for easier manipulation\n",
    "    \n",
    "    for _ in range(max_new_tokens):\n",
    "        # Create input tensor from current sequence\n",
    "        input_tensor = torch.tensor([generated_ids]).to(device)\n",
    "        \n",
    "        # Get logits for next token\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_tensor)\n",
    "            logits = outputs.logits[0, -1, :]  # (vocab_size,)\n",
    "        \n",
    "        # GREEDY: Pick the token with highest logit\n",
    "        next_token_id = torch.argmax(logits).item()\n",
    "        \n",
    "        # Append to sequence\n",
    "        generated_ids.append(next_token_id)\n",
    "        \n",
    "        # Stop if we hit end of text token\n",
    "        if next_token_id == tokenizer.eos_token_id:\n",
    "            break\n",
    "    \n",
    "    # Decode back to text\n",
    "    return tokenizer.decode(generated_ids)\n",
    "\n",
    "# Test greedy decoding\n",
    "prompt = \"Once upon a time\"\n",
    "print(f\"Prompt: '{prompt}'\")\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"GREEDY OUTPUT:\")\n",
    "print(\"=\"*50)\n",
    "output = greedy_decode(prompt, max_new_tokens=50)\n",
    "print(output)\n",
    "\n",
    "print(\"\\n\" + \"-\"*50)\n",
    "print(\"Note: Greedy is deterministic - same input always gives same output\")\n",
    "print(\"Run it again - you'll get the exact same text!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "temperature-header",
   "metadata": {},
   "source": [
    "---\n",
    "## Strategy 2: Temperature Sampling\n",
    "\n",
    "**Idea**: Scale logits before converting to probabilities.\n",
    "\n",
    "```python\n",
    "probs = softmax(logits / temperature)\n",
    "next_token = sample from probs\n",
    "```\n",
    "\n",
    "| Temperature | Effect |\n",
    "|-------------|--------|\n",
    "| T < 1 | Sharper distribution (more deterministic) |\n",
    "| T = 1 | Original distribution |\n",
    "| T > 1 | Flatter distribution (more random) |\n",
    "| T → 0 | Approaches greedy |\n",
    "| T → ∞ | Uniform random |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "temperature",
   "metadata": {},
   "outputs": [],
   "source": [
    "def temperature_decode(prompt: str, temperature: float = 1.0, max_new_tokens: int = 50) -> str:\n",
    "    \"\"\"\n",
    "    Temperature sampling: scale logits before softmax.\n",
    "    \n",
    "    Algorithm:\n",
    "    1. Get logits for next token\n",
    "    2. Divide logits by temperature\n",
    "    3. Apply softmax to get probabilities\n",
    "    4. Sample from the distribution\n",
    "    5. Repeat\n",
    "    \"\"\"\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
    "    generated_ids = input_ids[0].tolist()\n",
    "    \n",
    "    for _ in range(max_new_tokens):\n",
    "        input_tensor = torch.tensor([generated_ids]).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_tensor)\n",
    "            logits = outputs.logits[0, -1, :]\n",
    "        \n",
    "        # TEMPERATURE: Scale logits\n",
    "        scaled_logits = logits / temperature\n",
    "        \n",
    "        # Convert to probabilities\n",
    "        probs = F.softmax(scaled_logits, dim=-1)\n",
    "        \n",
    "        # Sample from the distribution\n",
    "        next_token_id = torch.multinomial(probs, num_samples=1).item()\n",
    "        \n",
    "        generated_ids.append(next_token_id)\n",
    "        \n",
    "        if next_token_id == tokenizer.eos_token_id:\n",
    "            break\n",
    "    \n",
    "    return tokenizer.decode(generated_ids)\n",
    "\n",
    "# Compare different temperatures\n",
    "prompt = \"The meaning of life is\"\n",
    "print(f\"Prompt: '{prompt}'\\n\")\n",
    "\n",
    "for temp in [0.3, 0.7, 1.0, 1.5]:\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"TEMPERATURE = {temp}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    output = temperature_decode(prompt, temperature=temp, max_new_tokens=40)\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "topp-header",
   "metadata": {},
   "source": [
    "---\n",
    "## Strategy 4: Top-p (Nucleus) Sampling\n",
    "\n",
    "**Idea**: Sample from the smallest set of tokens whose cumulative probability ≥ p.\n",
    "\n",
    "```python\n",
    "sorted_probs = sort probabilities descending\n",
    "cumulative = cumsum(sorted_probs)\n",
    "cutoff = first index where cumulative >= p\n",
    "keep tokens up to cutoff\n",
    "```\n",
    "\n",
    "**Why better than top-k?** The number of tokens varies based on the distribution:\n",
    "- Confident prediction → few tokens\n",
    "- Uncertain prediction → more tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "topp",
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_p_decode(prompt: str, p: float = 0.9, temperature: float = 1.0, max_new_tokens: int = 50) -> str:\n",
    "    \"\"\"\n",
    "    Top-p (nucleus) sampling: sample from smallest set with cumulative prob >= p.\n",
    "    \n",
    "    Algorithm:\n",
    "    1. Get logits and convert to probabilities\n",
    "    2. Sort probabilities in descending order\n",
    "    3. Compute cumulative sum\n",
    "    4. Find cutoff where cumsum >= p\n",
    "    5. Keep only tokens up to cutoff\n",
    "    6. Renormalize and sample\n",
    "    \"\"\"\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
    "    generated_ids = input_ids[0].tolist()\n",
    "    \n",
    "    for _ in range(max_new_tokens):\n",
    "        input_tensor = torch.tensor([generated_ids]).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_tensor)\n",
    "            logits = outputs.logits[0, -1, :]\n",
    "        \n",
    "        # Apply temperature first\n",
    "        scaled_logits = logits / temperature\n",
    "        \n",
    "        # Convert to probabilities\n",
    "        probs = F.softmax(scaled_logits, dim=-1)\n",
    "        \n",
    "        # TOP-P: Sort probabilities descending\n",
    "        sorted_probs, sorted_indices = torch.sort(probs, descending=True)\n",
    "        \n",
    "        # Compute cumulative sum\n",
    "        cumsum_probs = torch.cumsum(sorted_probs, dim=-1)\n",
    "        \n",
    "        # Find cutoff: where cumsum first exceeds p\n",
    "        # We want to KEEP tokens where cumsum <= p (plus the first one that exceeds)\n",
    "        sorted_indices_to_remove = cumsum_probs > p\n",
    "        # Shift right to keep the first token that exceeds p\n",
    "        sorted_indices_to_remove[1:] = sorted_indices_to_remove[:-1].clone()\n",
    "        sorted_indices_to_remove[0] = False\n",
    "        \n",
    "        # Set removed tokens to 0 probability\n",
    "        sorted_probs[sorted_indices_to_remove] = 0\n",
    "        \n",
    "        # Renormalize\n",
    "        sorted_probs = sorted_probs / sorted_probs.sum()\n",
    "        \n",
    "        # Create full probability tensor\n",
    "        filtered_probs = torch.zeros_like(probs)\n",
    "        filtered_probs[sorted_indices] = sorted_probs\n",
    "        \n",
    "        # Sample\n",
    "        next_token_id = torch.multinomial(filtered_probs, num_samples=1).item()\n",
    "        \n",
    "        generated_ids.append(next_token_id)\n",
    "        \n",
    "        if next_token_id == tokenizer.eos_token_id:\n",
    "            break\n",
    "    \n",
    "    return tokenizer.decode(generated_ids)\n",
    "\n",
    "# Compare different p values\n",
    "prompt = \"The secret to happiness is\"\n",
    "print(f\"Prompt: '{prompt}'\\n\")\n",
    "\n",
    "for p_val in [0.5, 0.8, 0.95, 0.99]:\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"TOP-P = {p_val}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    output = top_p_decode(prompt, p=p_val, temperature=0.8, max_new_tokens=40)\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beam-header",
   "metadata": {},
   "source": [
    "---\n",
    "## Strategy 5: Beam Search\n",
    "\n",
    "**Idea**: Track multiple candidate sequences (\"beams\") and keep the best ones.\n",
    "\n",
    "```\n",
    "Step 1: Start with n beams, each with the prompt\n",
    "Step 2: For each beam, get top-k next tokens\n",
    "Step 3: Score all (beam × token) combinations\n",
    "Step 4: Keep top n combinations as new beams\n",
    "Step 5: Repeat until done\n",
    "```\n",
    "\n",
    "**Scoring**: Usually sum of log-probabilities (higher = better)\n",
    "\n",
    "**Pros**: Finds higher probability sequences  \n",
    "**Cons**: Slower, can be repetitive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beam",
   "metadata": {},
   "outputs": [],
   "source": "def beam_search_decode(prompt: str, num_beams: int = 3, max_new_tokens: int = 50) -> str:\n    \"\"\"\n    Beam search: track multiple candidate sequences.\n    \n    Each beam is a tuple: (token_ids, cumulative_log_prob)\n    \n    Algorithm:\n    1. Start with one beam containing the prompt\n    2. For each beam, get log-probs for all next tokens\n    3. Consider all (beam, next_token) combinations\n    4. Keep top num_beams combinations by cumulative log-prob\n    5. Repeat until all beams hit EOS or max length\n    6. Return best beam\n    \"\"\"\n    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n    initial_ids = input_ids[0].tolist()\n    \n    # Initialize beams: (token_ids, cumulative_log_prob)\n    beams = [(initial_ids, 0.0)]\n    completed_beams = []\n    \n    for step in range(max_new_tokens):\n        all_candidates = []\n        \n        for beam_ids, beam_score in beams:\n            # Skip if this beam is already complete\n            if beam_ids[-1] == tokenizer.eos_token_id:\n                completed_beams.append((beam_ids, beam_score))\n                continue\n            \n            # Get next token probabilities\n            input_tensor = torch.tensor([beam_ids]).to(device)\n            with torch.no_grad():\n                outputs = model(input_tensor)\n                logits = outputs.logits[0, -1, :]\n            \n            # Convert to log-probabilities\n            log_probs = F.log_softmax(logits, dim=-1)\n            \n            # Get top candidates for this beam\n            top_log_probs, top_indices = torch.topk(log_probs, num_beams * 2)\n            \n            for log_prob, idx in zip(top_log_probs, top_indices):\n                new_ids = beam_ids + [idx.item()]\n                new_score = beam_score + log_prob.item()\n                all_candidates.append((new_ids, new_score))\n        \n        # If no active candidates, we're done\n        if not all_candidates:\n            break\n        \n        # Keep top num_beams candidates\n        all_candidates.sort(key=lambda x: x[1], reverse=True)\n        beams = all_candidates[:num_beams]\n    \n    # Add remaining beams to completed\n    completed_beams.extend(beams)\n    \n    # Return best beam (highest score)\n    best_beam = max(completed_beams, key=lambda x: x[1])\n    return tokenizer.decode(best_beam[0])\n\n# Test beam search\nprompt = \"The quick brown fox\"\nprint(f\"Prompt: '{prompt}'\\n\")\n\nprint(\"=\"*50)\nprint(\"GREEDY:\")\nprint(\"=\"*50)\nprint(greedy_decode(prompt, max_new_tokens=30))\n\nprint(f\"\\n{'='*50}\")\nprint(f\"BEAM SEARCH (num_beams=3):\")\nprint(f\"{'='*50}\")\nprint(beam_search_decode(prompt, num_beams=3, max_new_tokens=30))"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}