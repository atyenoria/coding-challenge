{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enhanced Greedy Decoder for Text Generation\n",
    "\n",
    "This notebook demonstrates a simple language model with an enhanced greedy decoder that includes:\n",
    "- Temperature scaling for controlling randomness\n",
    "- Repetition penalty to avoid repeated tokens\n",
    "- Statistical tracking of generation quality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import List, Dict, Tuple\n",
    "import re\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SimpleTokenizer\n",
    "\n",
    "A basic tokenizer with a predefined vocabulary of common English words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizer:\n",
    "    def __init__(self):\n",
    "        self.vocab = set([\n",
    "            # Common words\n",
    "            \"the\", \"a\", \"an\", \"and\", \"or\", \"but\", \"in\", \"on\", \"at\", \"to\",\n",
    "            \"is\", \"are\", \"was\", \"were\", \"will\", \"would\", \"could\", \"should\",\n",
    "            \"I\", \"you\", \"he\", \"she\", \"it\", \"we\", \"they\",\n",
    "            # Nouns\n",
    "            \"cat\", \"dog\", \"house\", \"tree\", \"car\", \"book\", \"city\", \"world\",\n",
    "            \"bird\", \"fish\", \"boy\", \"girl\", \"man\", \"woman\", \"child\",\n",
    "            # Verbs\n",
    "            \"run\", \"jump\", \"eat\", \"sleep\", \"read\", \"write\", \"speak\", \"think\",\n",
    "            \"walk\", \"see\", \"hear\", \"feel\", \"like\", \"love\", \"hate\",\n",
    "            # Adjectives\n",
    "            \"big\", \"small\", \"happy\", \"sad\", \"fast\", \"slow\", \"good\", \"bad\",\n",
    "            \"hot\", \"cold\", \"new\", \"old\", \"young\", \"tall\", \"short\",\n",
    "            # Punctuation\n",
    "            \",\", \".\", \"!\", \"?\", \" \"\n",
    "        ])\n",
    "        \n",
    "        self.special_tokens = {\n",
    "            \"pad\": \"<pad>\",\n",
    "            \"unk\": \"<unk>\",\n",
    "            \"bos\": \"<bos>\",\n",
    "            \"eos\": \"<eos>\"\n",
    "        }\n",
    "        self.vocab.update(self.special_tokens.values())\n",
    "        self._create_mappings()\n",
    "    \n",
    "    def _create_mappings(self):\n",
    "        self.token2id = {token: idx for idx, token in enumerate(sorted(self.vocab))}\n",
    "        self.id2token = {idx: token for token, idx in self.token2id.items()}\n",
    "        \n",
    "    def encode(self, text: str) -> List[int]:\n",
    "        tokens = re.findall(r'\\w+|[^\\w\\s]', text.lower())\n",
    "        return [self.token2id.get(token, self.token2id[self.special_tokens[\"unk\"]]) \n",
    "                for token in tokens]\n",
    "    \n",
    "    def decode(self, ids: List[int]) -> str:\n",
    "        tokens = [self.id2token.get(id, self.special_tokens[\"unk\"]) for id in ids]\n",
    "        text = \" \".join(tokens)\n",
    "        return re.sub(r'\\s+([,.!?])', r'\\1', text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MockLanguageModel\n",
    "\n",
    "A mock language model that uses predefined patterns to generate probability distributions for next tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MockLanguageModel:\n",
    "    def __init__(self, tokenizer: SimpleTokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.vocab_size = len(tokenizer.token2id)\n",
    "        \n",
    "        # Define common word patterns for more realistic text\n",
    "        self.patterns = {\n",
    "            \"the\": [\"cat\", \"dog\", \"house\", \"tree\", \"car\", \"book\", \"bird\", \"fish\"],\n",
    "            \"is\": [\"big\", \"small\", \"happy\", \"sad\", \"good\", \"bad\", \"hot\", \"cold\"],\n",
    "            \"was\": [\"running\", \"jumping\", \"reading\", \"sleeping\", \"walking\"],\n",
    "            \"cat\": [\"is\", \"was\", \"and\", \"likes\", \"runs\", \"sleeps\"],\n",
    "            \"dog\": [\"is\", \"was\", \"and\", \"likes\", \"runs\", \"barks\"],\n",
    "            \"they\": [\"are\", \"were\", \"will\", \"could\", \"should\", \"might\"],\n",
    "            \"i\": [\"am\", \"was\", \"will\", \"could\", \"should\", \"might\"],\n",
    "            \"a\": [\"big\", \"small\", \"happy\", \"sad\", \"good\", \"bad\", \"new\", \"old\"],\n",
    "        }\n",
    "        \n",
    "        # Add more sophisticated transitions\n",
    "        self.context_patterns = {\n",
    "            (\"the\", \"cat\"): [\"is\", \"was\", \"likes\", \"runs\"],\n",
    "            (\"is\", \"very\"): [\"happy\", \"sad\", \"big\", \"small\", \"good\", \"bad\"],\n",
    "            (\"they\", \"are\"): [\"happy\", \"sad\", \"good\", \"bad\", \"running\", \"sleeping\"],\n",
    "        }\n",
    "    \n",
    "    def get_next_token_probs(self, input_ids: List[int]) -> np.ndarray:\n",
    "        probs = np.ones(self.vocab_size) * 0.01  # Base probability\n",
    "        \n",
    "        if input_ids:\n",
    "            last_word = self.tokenizer.id2token[input_ids[-1]]\n",
    "            \n",
    "            # Single token patterns\n",
    "            if last_word in self.patterns:\n",
    "                for word in self.patterns[last_word]:\n",
    "                    if word in self.tokenizer.token2id:\n",
    "                        probs[self.tokenizer.token2id[word]] = 0.3\n",
    "            \n",
    "            # Context-based patterns (last two tokens)\n",
    "            if len(input_ids) >= 2:\n",
    "                last_two = tuple(self.tokenizer.id2token[id] for id in input_ids[-2:])\n",
    "                if last_two in self.context_patterns:\n",
    "                    for word in self.context_patterns[last_two]:\n",
    "                        if word in self.tokenizer.token2id:\n",
    "                            probs[self.tokenizer.token2id[word]] = 0.4\n",
    "            \n",
    "            # Add controlled randomness\n",
    "            probs += np.random.uniform(0, 0.1, size=self.vocab_size)\n",
    "            \n",
    "            # Reduce immediate repetition\n",
    "            if len(input_ids) > 1:\n",
    "                probs[input_ids[-1]] *= 0.1\n",
    "        \n",
    "        return probs / probs.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EnhancedGreedyDecoder\n",
    "\n",
    "An enhanced decoder with temperature scaling and repetition penalty for better text generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnhancedGreedyDecoder:\n",
    "    def __init__(self, model: MockLanguageModel, tokenizer: SimpleTokenizer, \n",
    "                 max_length: int = 20, temperature: float = 1.0,\n",
    "                 repetition_penalty: float = 1.2):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.temperature = temperature\n",
    "        self.repetition_penalty = repetition_penalty\n",
    "        \n",
    "        # Track generation statistics\n",
    "        self.stats = {\n",
    "            \"total_tokens\": 0,\n",
    "            \"unique_tokens\": 0,\n",
    "            \"repetitions\": 0,\n",
    "            \"pattern_matches\": 0\n",
    "        }\n",
    "    \n",
    "    def _apply_temperature(self, probs: np.ndarray) -> np.ndarray:\n",
    "        \"\"\" Apply temperature scaling to adjust randomness\"\"\"\n",
    "        logits = np.log(probs + 1e-10)  # Add epsilon for numerical stability\n",
    "        logits = logits / self.temperature\n",
    "        exp_logits = np.exp(logits - np.max(logits))  # Subtract max for stability\n",
    "        return exp_logits / exp_logits.sum()\n",
    "    \n",
    "    def _apply_repetition_penalty(self, probs: np.ndarray, input_ids: List[int]) -> np.ndarray:\n",
    "        \"\"\"Penalize recently used tokens\"\"\"\n",
    "        penalty_window = min(10, len(input_ids))  # Look at last 10 tokens\n",
    "        recent_tokens = set(input_ids[-penalty_window:])\n",
    "        \n",
    "        penalized_probs = probs.copy()\n",
    "        for token in recent_tokens:\n",
    "            penalized_probs[token] = penalized_probs[token] / self.repetition_penalty\n",
    "        \n",
    "        return penalized_probs / penalized_probs.sum()\n",
    "    \n",
    "    def _get_next_token(self, input_ids: List[int]) -> int:\n",
    "        \"\"\"Get next token with temperature and repetition penalty\"\"\"\n",
    "        # Get base probabilities\n",
    "        probs = self.model.get_next_token_probs(input_ids)\n",
    "        \n",
    "        # Apply repetition penalty\n",
    "        probs = self._apply_repetition_penalty(probs, input_ids)\n",
    "        \n",
    "        # Apply temperature\n",
    "        probs = self._apply_temperature(probs)\n",
    "        \n",
    "        # Sample from distribution\n",
    "        return int(np.random.choice(len(probs), p=probs))\n",
    "    \n",
    "    def generate(self, prompt: str, min_length: int = 5) -> str:\n",
    "        \"\"\"Generate text with enhanced features\"\"\"\n",
    "        input_ids = self.tokenizer.encode(prompt)\n",
    "        \n",
    "        # Reset statistics\n",
    "        self.stats = {\n",
    "            \"total_tokens\": len(input_ids),\n",
    "            \"unique_tokens\": len(set(input_ids)),\n",
    "            \"repetitions\": 0,\n",
    "            \"pattern_matches\": 0\n",
    "        }\n",
    "        \n",
    "        consecutive_repeats = 0\n",
    "        max_consecutive_repeats = 3\n",
    "        \n",
    "        while len(input_ids) < self.max_length:\n",
    "            next_token = self._get_next_token(input_ids)\n",
    "            \n",
    "            # Update statistics\n",
    "            self.stats[\"total_tokens\"] += 1\n",
    "            self.stats[\"unique_tokens\"] = len(set(input_ids + [next_token]))\n",
    "            \n",
    "            # Check for repetition\n",
    "            if len(input_ids) > 0 and next_token == input_ids[-1]:\n",
    "                consecutive_repeats += 1\n",
    "                self.stats[\"repetitions\"] += 1\n",
    "                if consecutive_repeats >= max_consecutive_repeats:\n",
    "                    break\n",
    "            else:\n",
    "                consecutive_repeats = 0\n",
    "            \n",
    "            # Check for pattern matches\n",
    "            if len(input_ids) > 0:\n",
    "                last_word = self.tokenizer.id2token[input_ids[-1]]\n",
    "                next_word = self.tokenizer.id2token[next_token]\n",
    "                if last_word in self.model.patterns and next_word in self.model.patterns[last_word]:\n",
    "                    self.stats[\"pattern_matches\"] += 1\n",
    "            \n",
    "            input_ids.append(next_token)\n",
    "            \n",
    "            # Stop conditions\n",
    "            if len(input_ids) >= min_length:\n",
    "                # Stop on punctuation after minimum length\n",
    "                if next_token in [self.tokenizer.token2id[\".\"], \n",
    "                                self.tokenizer.token2id[\"!\"],\n",
    "                                self.tokenizer.token2id[\"?\"]]:\n",
    "                    break\n",
    "            \n",
    "            # Force stop on EOS\n",
    "            if next_token == self.tokenizer.token2id[self.tokenizer.special_tokens[\"eos\"]]:\n",
    "                break\n",
    "        \n",
    "        return self.tokenizer.decode(input_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization Function\n",
    "\n",
    "Function to visualize multiple generations with statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_generations(decoder: EnhancedGreedyDecoder, prompt: str, num_samples: int = 5):\n",
    "    \"\"\"Visualize multiple generations with statistics\"\"\"\n",
    "    generations = []\n",
    "    stats = []\n",
    "    \n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    # Generate samples\n",
    "    for i in range(num_samples):\n",
    "        text = decoder.generate(prompt)\n",
    "        generations.append(text)\n",
    "        stats.append(decoder.stats.copy())\n",
    "    \n",
    "    # Plot statistics\n",
    "    metrics = [\"total_tokens\", \"unique_tokens\", \"repetitions\", \"pattern_matches\"]\n",
    "    for i, metric in enumerate(metrics):\n",
    "        plt.subplot(2, 2, i+1)\n",
    "        values = [s[metric] for s in stats]\n",
    "        plt.bar(range(num_samples), values)\n",
    "        plt.title(f\"{metric.replace('_', ' ').title()}\")\n",
    "        plt.xlabel(\"Sample\")\n",
    "        plt.ylabel(\"Count\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print generations\n",
    "    print(\"\\nGenerated Samples:\")\n",
    "    print(\"-\" * 50)\n",
    "    for i, text in enumerate(generations):\n",
    "        print(f\"Sample {i+1}: {text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Components\n",
    "\n",
    "Create the tokenizer and model instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize components\n",
    "tokenizer = SimpleTokenizer()\n",
    "model = MockLanguageModel(tokenizer)\n",
    "\n",
    "print(f\"Vocabulary size: {len(tokenizer.token2id)}\")\n",
    "print(f\"Sample tokens: {list(tokenizer.token2id.keys())[:10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Different Configurations\n",
    "\n",
    "Compare text generation with different temperature and repetition penalty settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different configurations\n",
    "print(\"Testing Different Configurations:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "configs = [\n",
    "    (\"Conservative\", {\"temperature\": 0.5, \"repetition_penalty\": 1.5}),\n",
    "    (\"Balanced\", {\"temperature\": 1.0, \"repetition_penalty\": 1.2}),\n",
    "    (\"Creative\", {\"temperature\": 1.5, \"repetition_penalty\": 1.1}),\n",
    "]\n",
    "\n",
    "prompts = [\n",
    "    \"the cat is\",\n",
    "    \"i feel\",\n",
    "    \"they were\",\n",
    "    \"a small dog\"\n",
    "]\n",
    "\n",
    "for name, params in configs:\n",
    "    print(f\"\\n{name} Configuration:\")\n",
    "    decoder = EnhancedGreedyDecoder(model, tokenizer, **params)\n",
    "    \n",
    "    for prompt in prompts:\n",
    "        print(f\"\\nPrompt: {prompt}\")\n",
    "        for _ in range(3):  # Generate 3 samples\n",
    "            output = decoder.generate(prompt)\n",
    "            print(f\"Generated: {output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detailed Analysis\n",
    "\n",
    "Visualize generation statistics for the balanced configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize generations for one configuration\n",
    "print(\"Detailed Analysis of Balanced Configuration:\")\n",
    "balanced_decoder = EnhancedGreedyDecoder(model, tokenizer, \n",
    "                                       temperature=1.0, \n",
    "                                       repetition_penalty=1.2)\n",
    "\n",
    "visualize_generations(balanced_decoder, \"the cat is\", num_samples=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
